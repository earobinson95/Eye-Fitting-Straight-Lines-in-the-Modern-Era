---
  title: Eye Fitting Straight Lines in the Modern Era

# to produce blinded version set to 1
blinded: 0

authors: 
  - name: Emily A. Robinson 1
#thanks: The authors gratefully acknowledge ...
affiliation: Department of Statistics, University of Nebraska - Lincoln

- name: Susan VanderPlas 2
affiliation: Department of Statistics, University of Nebraska - Lincoln

- name: Reka Howard 3
affiliation: Department of Statistics, University of Nebraska - Lincoln

keywords:
  - Graphics
- Regression
- Graph Perception
- Scatterplot
- Cognitive Bias

abstract: |
  Fitting lines by eye through a set of points has been explored since the 20th century. Common methods of fitting trends by eye involve maneuvering a string, black thread, or ruler until the fit is suitable, then drawing the line through the set of points. In 2015, the New York Times introduced an interactive feature, called 'You Draw It'. Readers are asked to input their own assumptions about various metrics and compare how these assumptions relate to reality. The New York Times team utilizes Data Driven Documents (D3) that allows readers to predict these metrics by drawing a line on their computer screen with their computer mouse. In my research, I established ‘You Draw It’ as a method for graphical testing by adapting the New York Times feature. I recruited participants via crowdsourcing websites and replicated the study found in Eye Fitting Straight Lines (Mosteller et al., 1981). Participants were directed to an RShiny application link and shown points following a linear trend and asked to draw a line through the data points using their computer mouse; task plots were generated using the r2d3 package in R statistical software. Results from my study were consistent with those found in the previous study; when shown points following a linear trend, participants tended to fit the slope of the first principal component over the slope of the least-squares regression line. This trend was most prominent when shown data simulated with larger variances. The reproducibility of these results serves as evidence of the reliability of the you draw it method. Future work is necessary to implement the 'You Draw It' tool as a method of testing graphics. [200 word limit]

bibliography: bibliography.bib
output: rticles::asa_article

header-includes:
  - \newcommand{\ear}[1]{{\textcolor{blue}{#1}}}
    - \newcommand{\svp}[1]{{\textcolor{RedOrange}{#1}}}
      - \newcommand{\rh}[1]{{\textcolor{Green}{#1}}}
        - \usepackage[capitalise]{cleveref}
        - \newcommand\pcref[1]{(\cref{#1})}
          - \usepackage{algorithm,algpseudocode,booktabs}
          ---
            
            ```{r setup, include = F}
          knitr::opts_chunk$set(
            echo = F, eval = T, messages = F, warnings = F,
            fig.width = 6, fig.height = 4,  fig.align = 'center',
            out.width = "\\linewidth", dpi = 300, 
            tidy = T, tidy.opts=list(width.cutoff=45),
            fig.pos = "tbp",
            out.extra = ""
          )
          ```
          
          ```{r load-packages, include = F}
          library(readr)
          library(tidyverse)
          library(knitr)
          library(gridExtra)
          library(patchwork)
          library(cowplot)
          library(ggforce)
          ```
          
          # Introduction
          Advanced technology and computing power have promoted data visualization as a central tool in modern data science. @unwin2020data defines data visualization as the art of drawing graphical charts in order to display data. Graphics are useful for data cleaning, exploring data structure, and have been an essential component in communicating information for the last 200 years [@lewandowsky1989perception]. Although statistical graphics have become widely used and valued in science, business, and in many other aspects of life, as creators of graphics, we are too accepting of them as default without asking critical questions about the graphics we create or view (Unwin, 2020). 
          
          ## Graph Perception
          
          
          ## Testing Statistical Graphics
          
          One way in which we evaluate the effectiveness of charts is through the use of graphical tests. We could ask participants to identify differences in graphs, read information off of a chart accurately, use data to make correct real-world decisions, or predict the next few observations. All of these types of tests require different levels of use and manipulation of the information being presented in the chart.
          
          + Psychophysics (speed and accuracy)
          + Lineups
          
          ## Trend Judgement
          
          + @finney1951subjective: "rule two lines"
          + @mosteller1981eye: transparancy sheet with a line drawn across middle.
          + @ciccione2021can: used a trackpad to adjust the tilt of a line on screen.
          
          + Ensemble Perception: the visual system can compute averages of various features in parallel across the items in a set
          + Strength of Association: when participants are asked to judge the strength of an association presented in a scatterplot, their judgement is more accurate for higher levels of the Pearson correlation coefficient r, although still affected by the variance of the dataset.
          
          Initial studies in the 20th century explored the use of fitting lines by eye through a set of points [@finney1951subjective; @mosteller1981eye]. 
          Common methods of fitting trends by eye involve maneuvering a string, black thread, or ruler until the fit is suitable, then drawing the line through the set of points. 
          In @finney1951subjective, it was of interest to determine the effect of stopping iterative maximum likelihood calculations after one iteration. 
          Many techniques in statistical analysis are performed with the aid of iterative calculations such as Newton's method or Fisher's scoring. 
          Guesses are made at the best estimates of certain parameters and these guesses are then used as the basis of a computation which yields a new set of approximation to the parameter estimates; this same procedure is the performed on the new parameter estimates and the computing cycle is repeated until convergence, as determined by the statistician, is reached. 
          The author was interested in whether one iteration of calculations was sufficient in the estimation of parameters connected with dose-response relationships. 
          One measure of interest is the relative potency between a test preparation of doses and standard preparation of does; relative potency is calculated as the ratio of two equally effective doses between the two preparation methods. 
          \cref{fig:subjective-judgement} shows a pair of parallel probit responses in a biological assay.
          The x-axis is the $\log_{1.5}$ dose level for four dose levels (for example, doses 4, 6, 9, and 13 correspond correspond to equally spaced values on a logarithmic scale, labeled  0, 1, 2, and 3) and the y-axis is the corresponding probit response as calculated in @finney1948table; circles correspond to the test preparation method while the crosses correspond to the standard preparation method.
          For these sort of assays, the does-response relationship follows a linear regression of the probit response on the logarithm of the dose levels; the two preparation methods can be constrained to be parallel [@jerne1949validity], limiting the relative potency to one consistent value. 
          In this study, twenty-one scientists were recruited via postal mail and asked to "rule two lines" in order to judge by eye the positions for a pair of parallel probit regression lines in a biological assay.
          <!-- https://www.quantics.co.uk/blog/what-is-relative-potency/ -->
            The author then computed one iterative calculation of the relative potency based on starting values as indicated by the pair of lines provided by each participant and compared these relative potency estimates to that which was estimated by the full probit technique (reaching convergence through multiple iterations).
          Results indicated that one cycle of iterations for calculating the relative potency was sufficient based on the starting values provided by eye from the participants.
          
          Thirty years later, @mosteller1981eye, sought to understand the properties of least squares and other computed lines by establishing one systematic method of fitting lines by eye. 
          The authors recruited 153 graduate students and post doctoral researchers in Introductory Biostatistics. 
          Participants were asked to fit lines by eye to four scatterplots using an 8.5 x 11 inch transparency with a straight line etched completely across the middle. 
          A latin square design [@giesbrecht2004planning] with packets of the set of points stapled together in four different sequences was used to determine if there is an effect of order of presentation.
          It was found that order of presentation had no effect and that participants tended to fit the slope of the first principal component (error minimized orthogonally, both horizontal and vertical, to the regression line) over the slope of the least squares regression line (error minimized vertically to the regression line).
          
          In 2015, the New York Times introduced an interactive feature, called You Draw It [@aisch_cox_quealy_2015; @buchanan_park_pearce_2017; @katz_2017].
          Readers are asked to input their own assumptions about various metrics and compare how these assumptions relate to reality.
          The New York Times team utilizes Data Driven Documents (D3) that allows readers to predict these metrics through the use of drawing a line on their computer screen with their computer mouse. 
          [@katz_2017] is one such example in which readers are asked to draw the line for the missing years providing what they estimate to be the number of Americans who have died every year from car accidents, since 1990.
          After the reader has completed drawing the line, the actual observed values are revealed and the reader may check their estimated knowledge against the actual reported data.
          
          ```{r nyt-caraccidents, include = F, fig.cap = "New York Times You Draw It Feature", out.width="75%"}
          knitr::include_graphics("images/nyt-caraccidents-frame4.png")
          ```
          
          Major news and research organizations such as the New York Times, FiveThirtyEight, Washington Post, and the Pew Research Center create and customize graphics with Data Driven Documents (D3).
          In June 2020, the New York Times released a front page displaying figures that represent each of the 100,000 lives lost from the COVID-19 pandemic until this point in time [@NYTrememberinglives]; this visualization was meant to bring about a visceral reaction and resonate with readers. 
          During 2021 March Madness, FiveThirtyEight created a roster-shuffling machine which allowed readers to build their own NBA contender through interactivity [@ryanabest_2021].
          Data Driven Documents (D3) is an open-source JavaScript based graphing framework created by Mike Bostock during his time working on graphics at the New York Times.
          For readers familiar with R, it is notable to consider D3 in JavaScript equivalent to the ggplot2 package in R [@ggplot2].
          
          ## Research objective
          
          The goal of this paper is to establish you draw it as a tool for measuring predictions of trends fitted by eye and a method for testing graphics.
          In order to validate you draw it as a method for testing graphics, the first sub-study, referred to as Eye Fitting Straight Lines in the Modern Era, replicated the experiment and results found in @mosteller1981eye. 
          
          # Methods
          
          ## Data Simulation
          
          All data processing was conducted in R before being passed to the D3.js source code. 
          A total of $N = 30$ points $(x_i, y_i), i = 1,...N$ were generated for $x_i \in [x_{min}, x_{max}]$ where $x$ and $y$ have a linear relationship.
          Data were simulated based on linear model with additive errors: 
            \begin{align}
          y_i & = \beta_0 + \beta_1 x_i + e_i \\
          \text{with } e_i & \sim N(0, \sigma^2). \nonumber
          \end{align} 
          The parameters $\beta_0$ and $\beta_1$ are selected to replicate @mosteller1981eye with $e_i$ generated by rejection sampling in order to guarantee the points shown align with that of the fitted line. 
          An ordinary least squares regression is then fit to the simulated points in order to obtain the best fit line and fitted values in 0.25 increments across the domain, $(x_k, \hat y_{k,OLS}), k = 1, ..., 4 x_{max} +1$.
          The data simulation function then outputs a list of point data and line data both indicating the parameter identification, x-value, and corresponding simulated or fitted y value.
          The data simulation procedure is described in \cref{alg:eyefitting-algorithm}.
          
          \begin{algorithm}
          \caption{Eye Fitting Straight Lines in the Modern Era Data Simulation}\label{alg:eyefitting-algorithm}
          \begin{algorithmic}[1]
          \Statex \textbullet~\textbf{Input Parameters:} $y_{\bar{x}}$ for calculating the y-intercept, $\beta_0$; slope $\beta_1$; standard deviation from line $\sigma$; sample size of points $N = 30$; domain $x_{min}$ and $x_{max}$; fitted value increment $x_{by} = 0.25$.
          \Statex \textbullet~\textbf{Output Parameters:} List of point data and line data each indicating the parameter identification, x value, and corresponding simulated or fitted y value.
          \State Randomly select and jitter $N = 30$ x-values along the domain, $x_{i=1:N}\in [x_{min}, x_{max}]$.
          \State Determine the y-intercept, $\beta_0$, at x = 0 from the provided slope ($\beta_1$) and y-value at the mean of x ($y_{\bar{x}}$) using point-slope equation of a line.
          \State Generate "good" errors, $e_{i = 1:N}$ based on $N(0,\sigma)$ by setting a constraint requiring the mean of the first $\frac{1}{3}\text{N}$ errors $< |2\sigma|.$
            \State Simulate point data based on $y_i = \beta_0 + \beta_1 x_i + e_i$
            \State Obtain ordinary least squares regression coefficients, $\hat\beta_0$ and $\hat\beta_1$, for the simulated point data using the lm function in the stats package in base R.
          \State Obtain fitted values every 0.25 increment across the domain from the ordinary least squares regression $\hat y_{k,OLS} = \hat\beta_{0,OLS} + \hat\beta_{1,OLS} x_k$.
          \State Output data list of point data and line data each indicating the parameter identification, x value, and corresponding simulated or fitted y value.
          \end{algorithmic}
          \end{algorithm}
          
          ```{r eyefitting-parameters}
          data.frame(Parm = c("F", "N", "S", "V"),
                     y_xbar = c(3.9, 4.11, 3.88, 3.89),
                     slope = c(0.66, -0.70, 0.66, 1.98),
                     sigma = c(1.98, 2.5, 1.3, 1.5)
          ) %>%
            mutate(Parm = factor(Parm, levels = c("S", "F", "V", "N"))) %>%
            arrange(Parm) %>%
            knitr::kable("latex", 
                         digits = 2, 
                         escape = F, 
                         booktabs = T, 
                         linesep = "", 
                         align = "c", 
                         label = "eyefitting-parameters",
                         col.names = c("Parameter Choice", "$y_{\\bar{x}}$", "$\\beta_1$", "$\\sigma$"),
                         caption = "Eye Fitting Straight Lines in the Modern Era simulation model parameters")
          ```
          
          ## Parameter selection
          
          Simulated model equation parameters were selected to reflect the four data sets (F, N, S, and V) used in @mosteller1981eye \pcref{tab:eyefitting-parameters}. 
          Parameter choices F, N, and S simulated data across a domain of 0 to 20. 
          Parameter choice F produces a trend with a positive slope and a large variance while N has a negative slope and a large variance. 
          In comparison, S shows a trend with a positive slope with a small variance and V yields a steep positive slope with a small variance over the domain of 4 to 16. 
          \cref{fig:eyefitting-simplot} illustrates an example of simulated data for all four parameter choices intended to reflect the trends seen in \cref{fig:mosteller-eyefitting-plot}.
          Aesthetic design choices were made consistent across each of the interactive you draw it plots; the aspect ratio, defining the $x$ to $y$ axis ratio was set to one and the y-axis range extended 10\% beyond (above and below) the range of the simulated data points to allow for users to draw outside the simulated data set range. 
          
          ```{r eyefitting-simplot, fig.height = 6, fig.width = 6, fig.cap = "Eye Fitting Straight Lines in the Modern Era Simulated Data Example", out.width="75%"}
          
          eyefitting_example_sim <- read.csv("data/youdrawit/youdrawit-eyefitting-simdata-example.csv")
          eyefitting_example_simplot <- eyefitting_example_sim %>%
            filter(data == "point_data") %>%
            filter(dataset %in% c("F", "N", "S") | (x < 16 & x > 4)) %>%
            mutate(dataset = factor(dataset, levels = c("S", "F", "V", "N"))) %>%
            dplyr::rename(`Parameter Choice` = dataset) %>%
            ggplot(aes(x = x, y = y)) +
            geom_point(size = 1) +
            facet_wrap(~`Parameter Choice`, labeller = labeller(`Parameter Choice` = label_both), ncol = 2) +
            theme_bw(base_size = 14) +
            theme(aspect.ratio = 1,
                  legend.position = "none",
                  plot.title   = element_text(size = 12, hjust = 0),
                  axis.text    = element_text(size = 12),
                  axis.title   = element_text(size = 12),
                  legend.title = element_text(size = 12),
                  legend.text  = element_text(size = 12),
                  # strip.text = element_text(size = 5, margin = margin(0.05,0,0.05,0, "cm")),
                  # strip.background = element_rect(size = 0.5),
                  legend.key.size = unit(1, "line")
            ) 
          
          eyefitting_example_simplot
          ```
          
          ## 'You Draw It' Task Plots
          
          ## Procedure
          
          + 2 practice trials.
          
          ## Participants
          
          + Recruted through Twitter, Reddit, and direct email (May 2021)
          + 39 individuals completed 256 unique 'You Draw It' task plots; all completed you draw it task plots were included in the analysis.
          + All participants had normal or corrected to normal vision.
          + IRB approval and informed consent.
          + Not paid.
          + Took approximately 15 minutes to complete.
          + Participants completed the experiment using a RShiny application found [here](https://shiny.srvanderplas.com/you-draw-it/).
          
          # Results
          
          + Because of the randomness in the stimulus generation process, on any given trial the actual slope of the linear regression line could differ from its prescribed value.
          
          In addition to the participant drawn points, $(x_k, y_{k,drawn})$, and the ordinary least squares (OLS) regression fitted values, $(x_k, \hat y_{k,OLS})$, a regression equation with a slope based on the first principal component (PCA) was used to calculate fitted values, $(x_k, \hat y_{k,PCA})$.
          For each set of simulated data and parameter choice, the PCA regression equation was determined by using the princomp function in the stats package in base R to obtain the rotation of the coordinate axes from the first principal component (direction which captures the most variance).
          The estimated slope, $\hat\beta_{1,PCA}$, is determined by the ratio of the axis rotation in y and axis rotation in x of the first principal component with the y-intercept, $\hat\beta_{0,PCA}$ calculated by the point-slope equation of a line using the mean of of the simulated points, $(\bar x_i, \bar y_i)$.
          <!-- \rh{Please explain x-rotation and y-rotation of PC1. Maybe the first paragraph of section 3.3.2 could be extended.} -->
            Fitted values, $\hat y_{k,PCA}$ are then obtained every 0.25 increment across the domain from the PCA regression equation, $\hat y_{k,PCA} = \hat\beta_{0,PCA} + \hat\beta_{1,PCA} x_k$.
          \cref{fig:ols-vs-pca-example} illustrates the difference between an OLS regression equation which minimizes the vertical distance of points from the line and a regression equation with a slope calculated by the first principal component which minimizes the smallest distance of points from the line.
          
          ```{r ols-vs-pca-example, fig.height = 6, fig.width = 8, fig.cap="OLS vs PCA Regression Lines", message=FALSE, warning=FALSE, out.width="100%"}
          library(ggplot2)
          library(magrittr)
          library(plyr)
          
          set.seed(2)
          corrCoef = 0.5 # sample from a multivariate normal, 10 datapoints
          dat = MASS::mvrnorm(10,c(0,0),Sigma = matrix(c(1,corrCoef,2,corrCoef),2,2))
          dat[,1] = dat[,1] - mean(dat[,1]) # it makes life easier for the princomp
          dat[,2] = dat[,2] - mean(dat[,2])
          
          dat = data.frame(x1 = dat[,1],x2 = dat[,2])
          
          # Calculate the first principle component
          # see http://stats.stackexchange.com/questions/13152/how-to-perform-orthogonal-regression-total-least-squares-via-pca
          v = dat%>%prcomp%$%rotation
          x1x2cor = bCor = v[2,1]/v[1,1]
          
          x1tox2 = coef(lm(x1~x2,dat))
          x2tox1 = coef(lm(x2~x1,dat))
          slopeData = data.frame(slope = c(x1x2cor,x2tox1[2]),
                                 type=c("Principal Component", "Ordinary Least Squares"))
          
          # We want this to draw the neat orthogonal lines.
          pointOnLine = function(inp){
            # y = a*x + c (c=0)
            # yOrth = -(1/a)*x + d
            # yOrth = b*x + d
            x0 = inp[1] 
            y0 = inp[2] 
            a = x1x2cor
            b = -(1/a)
            c = 0
            d = y0 - b*x0
            x = (d-c)/(a-b)
            y = -(1/a)*x+d
            return(c(x,y))
          }
          
          points = apply(dat,1,FUN=pointOnLine)
          
          segmeData = rbind(data.frame(x=dat[,1],y=dat[,2],xend=points[1,],yend=points[2,],type = "Principal Component"),
                            data.frame(x=dat[,1],y=dat[,2],yend=dat[,1]*x2tox1[2],xend=dat[,1],type="Ordinary Least Squares"))
          
          dat %>%
            ggplot(aes(x1,x2))+
            geom_point()+
            geom_abline(data=slopeData,aes(slope = slope,intercept=0,color=type, linetype=type), size = 1.2)+
            geom_segment(data=segmeData,aes(x=x,y=y,xend=xend,yend=yend,color=type, linetype=type))+
            facet_grid(.~type)+
            coord_equal()+
            scale_x_continuous("x") +
            scale_y_continuous("y") +
            theme_bw(base_size = 14) +
            theme(aspect.ratio = 1,
                  legend.position = "none",
                  axis.text    = element_text(size = 12),
                  axis.title   = element_text(size = 12),
                  legend.title = element_blank(),
                  # legend.text  = element_text(size = 10),
                  # strip.text = element_text(size = 8, margin = margin(0.1,0,0.1,0, "cm")),
                  # strip.background = element_rect(size = 0.8),
                  legend.key.size = unit(1, "line")
            ) +
            scale_color_manual(values = c("steelblue", "orange"), labels = c("OLS", "PCA")) +
            scale_linetype_manual(values = c("solid", "dashed"), labels = c("OLS", "PCA"))
          ```
          
          <!-- To calculate the first principal component fit: https://benediktehinger.de/blog/science/scatterplots-regression-lines-and-the-first-principal-component/ -->
            
            For each participant, the final data set used for analysis contains $x_{ijk}, y_{ijk,drawn}, \hat y_{ijk,OLS}$, and $\hat y_{ijk,PCA}$ for parameter choice $i = 1,2,3,4$, j = $1,...N_{participant}$, and $x_{ijk}$ value $k = 1, ...,4 x_{max} + 1$. 
          Using both a linear mixed model and a generalized additive mixed model, comparisons of vertical residuals in relation to the OLS fitted values ($e_{ijk,OLS} = y_{ijk,drawn} - \hat y_{ijk,OLS}$) and PCA fitted values ($e_{ijk,PCA} = y_{ijk,drawn} - \hat y_{ijk,PCA}$) were made across the domain.
          \cref{fig:eyefitting-example-plot} displays an example of all three fitted trend lines for parameter choice F.
          
          ```{r eyefitting-example-plot, fig.height = 4, fig.width = 4, fig.cap = "Eye Fitting Straight Lines in the Modern Era Example", out.width="65%"}
          trial.feedback <- read.csv("data/youdrawit/youdrawit-eyefitting-example-feedback.csv") %>%
            mutate(`Parameter Choice` = "F")
          trial.sim <- read.csv("data/youdrawit/youdrawit-eyefitting-example-simulated.csv") %>%
            mutate(`Parameter Choice` = "F")
          
          trial.feedback %>%
            ggplot(aes(x = x)) +
            geom_line(aes(y = y, color = "OLS", linetype = "OLS")) +
            geom_line(aes(y = ypca, color = "PCA", linetype = "PCA")) +
            geom_line(aes(y = ydrawn, color = "Drawn", linetype = "Drawn")) +
            geom_point(data = trial.sim, aes(y = y)) +
            facet_wrap(~`Parameter Choice`, labeller = labeller(`Parameter Choice` = label_both)) +
            theme_bw(base_size = 14) +
            theme(aspect.ratio = 1,
                  legend.position = "bottom",
                  axis.text    = element_text(size = 12),
                  axis.title   = element_text(size = 12),
                  legend.title = element_text(size = 12),
                  legend.text  = element_text(size = 12),
                  # strip.text = element_text(size = 8, margin = margin(0.1,0,0.1,0, "cm")),
                  # strip.background = element_rect(size = 0.8),
                  legend.key.size = unit(1, "line")
            ) +
            scale_x_continuous(limits = c(0,20)) +
            scale_color_manual("", values = c("black", "steelblue", "orange")) +
            scale_linetype_manual("", values = c("dashed", "solid", "solid"))
          ```
          
          Using the lmer function in the lme4 package [@lme4], a linear mixed model (LMM) is fit separately to the OLS and PCA residuals, constraining the fit to a linear trend. 
          Parameter choice, $x$, and the interaction between $x$ and parameter choice were treated as fixed effects with a random participant effect accounting for variation due to participant.
          The LMM equation for each fit (OLS and PCA) residuals is given by:
            \begin{equation}
          y_{ijk,drawn} - \hat y_{ijk,fit} = e_{ijk,fit} = \left[\gamma_0 + \alpha_i\right] + \left[\gamma_{1} x_{ijk} + \gamma_{2i} x_{ijk}\right] + p_{j} + \epsilon_{ijk}
          \end{equation}
          \noindent where
          
          + $y_{ijk,drawn}$ is the drawn y-value for the $i^{th}$ parameter choice, $j^{th}$ participant, and $k^{th}$ increment of x-value
          + $\hat y_{ijk,fit}$ is the fitted y-value for the $i^{th}$ parameter choice, $j^{th}$ participant, and $k^{th}$ increment of x-value corresponding to either the OLS or PCA fit
          + $e_{ijk,fit}$ is the residual between the drawn and fitted y-values for the $i^{th}$ parameter choice, $j^{th}$ participant, and $k^{th}$ increment of x-value corresponding to either the OLS or PCA fit
          + $\gamma_0$ is the overall intercept
          + $\alpha_i$ is the effect of the $i^{th}$ parameter choice (F, S, V, N) on the intercept
          + $\gamma_1$ is the overall slope for $x$
            + $\gamma_{2i}$ is the effect of the parameter choice on the slope
          + $x_{ijk}$ is the x-value for the $i^{th}$ parameter choice, $j^{th}$ participant, and $k^{th}$ increment
          + $p_{j} \sim N(0, \sigma^2_{participant})$ is the random error due to the $j^{th}$ participant's characteristics
+ $\epsilon_{ijk} \sim N(0, \sigma^2)$ is the residual error.

Eliminating the linear trend constraint, the bam function in the mgcv package [@mgcv1; @mgcv2; @mgcv3; @mgcv4; @mgcv5] is used to fit a generalized additive mixed model (GAMM) separately to the OLS and PCA residuals to allow for estimation of smoothing splines.
Parameter choice was treated as a fixed effect with no estimated intercept and a separate smoothing spline for $x$ was estimated for each parameter choice. A random participant effect accounting for variation due to participant and a random spline for each participant accounted for variation in spline for each participant.
The GAMM equation for each fit (OLS and PCA) residuals is given by:
\begin{equation}
y_{ijk, drawn} - \hat y_{ijk, fit} = e_{ijk,fit} = \alpha_i + s_{i}(x_{ijk}) + p_{j} + s_{j}(x_{ijk})
\end{equation}
\noindent where

+ $y_{ijk,drawn}$ is the drawn y-value for the $i^{th}$ parameter choice, $j^{th}$ participant, and $k^{th}$ increment of x-value
+ $\hat y_{ijk,fit}$ is the fitted y-value for the $i^{th}$ parameter choice, $j^{th}$ participant, and $k^{th}$ increment of x-value corresponding to either the OLS or PCA fit
+ $e_{ijk,fit}$ is the residual between the drawn and fitted y-values for the $i^{th}$ parameter choice, $j^{th}$ participant, and $k^{th}$ increment of x-value corresponding to either the OLS or PCA fit
+ $\alpha_i$ is the intercept for the parameter choice $i$
+ $s_{i}$ is the smoothing spline for the $i^{th}$ parameter choice
+ $x_{ijk}$ is the x-value for the $i^{th}$ parameter choice, $j^{th}$ participant, and $k^{th}$ increment
+ $p_{j} \sim N(0, \sigma^2_{participant})$ is the error due to participant variation
+ $s_{j}$ is the random smoothing spline for each participant.

\cref{fig:eyefitting-lmer-residualplots} and \cref{fig:eyefitting-gamm-residualplots} show the estimated trends of residuals (vertical deviation of participant drawn points from both the OLS and PCA fitted points) as modeled by a LMM and GAMM respectively. 
Examining the plots, the estimated trends of PCA residuals (orange) appear to align closer to the $y=0$ horizontal (dashed) line than the OLS residuals (blue). 
In particular, this trend is more prominent in parameter choices with large variances (F and N).
These results are consistent to those found in @mosteller1981eye indicating participants fit a trend line closer to the estimated regression line with the slope of the first principal component than the estimated OLS regression line.

```{r eyefitting-plots}
eyefitting_model_data <- read.csv("data/youdrawit/youdrawit-eyefitting-model-data.csv") %>%
  dplyr::rename(`Parameter Choice` = parm_id)
```

```{r eyefitting-lmer-residualplots, fig.height = 8, fig.width = 8, out.width = "90%", fig.cap = "Eye Fitting Straight Lines in the Modern Era LMM results"}
eyefitting.preds.lmer <- read.csv("data/youdrawit/youdrawit-eyefitting-lmerpred-data.csv")
# Plot Predictions
eyefitting.lmer.plot <- eyefitting.preds.lmer %>%
  filter((parm_id %in% c("F", "N", "S") | (x <= 16 & x >= 4))) %>%
  mutate(parm_id = factor(parm_id, levels = c("S", "F", "V", "N"))) %>%
  dplyr::rename(`Parameter Choice` = parm_id) %>%
  ggplot(aes(x = x)) +
  geom_line(data = eyefitting_model_data, aes(x = x, y = residualols, group = plotID, color = "OLS"), alpha = 0.1) +
  geom_line(data = eyefitting_model_data, aes(x = x, y = residualpca, group = plotID, color = "PCA"), alpha = 0.1) +
  geom_ribbon(aes(ymin = asymp.LCL.ols, ymax = asymp.UCL.ols, fill = "OLS"), color = NA, alpha = 0.7) +
  geom_line(aes(y = emmean.ols, color = "OLS")) +
  geom_ribbon(aes(ymin = asymp.LCL.pca, ymax = asymp.UCL.pca, fill = "PCA"), color = NA, alpha = 0.7) +
  geom_line(aes(y = emmean.pca, color = "PCA")) +
  geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.5) +
  facet_wrap(~`Parameter Choice`, labeller = labeller(`Parameter Choice` = label_both), scales = "free") +
  theme_bw(base_size = 14) +
  theme(aspect.ratio = 1,
        legend.position = "right",
        plot.title   = element_text(size = 12, hjust = 0),
        axis.text    = element_text(size = 12),
        axis.title   = element_text(size = 12),
        legend.title = element_text(size = 12),
        legend.text  = element_text(size = 12),
        # strip.text = element_text(size = 5, margin = margin(0.05,0,0.05,0, "cm")),
        # strip.background = element_rect(size = 0.5),
        legend.key.size = unit(1, "line")
        ) +
  scale_y_continuous("Residual") +
  scale_color_manual("Individual participant \nresiduals", values = c("steelblue", "orange"), labels = c("OLS", "PCA")) +
  scale_fill_manual("LMER fitted trend", values = c("steelblue", "orange"), labels = c("OLS", "PCA")) 

eyefitting.lmer.plot
```

```{r eyefitting-gamm-residualplots, fig.height = 8, fig.width = 8, out.width = "90%", fig.cap = "Eye Fitting Straight Lines in the Modern Era GAMM results"}
eyefitting.grid.gamm <- read.csv("data/youdrawit/youdrawit-eyefitting-gammpred-data.csv")
eyefitting.gamm.plot <- eyefitting.grid.gamm %>%
  filter((parm_id %in% c("F", "N", "S") | (x <= 16 & x >= 4))) %>%
  mutate(parm_id = factor(parm_id, levels = c("S", "F", "V", "N"))) %>%
  dplyr::rename(`Parameter Choice` = parm_id) %>%
  ggplot(aes(x = x)) +
  geom_line(data = eyefitting_model_data, aes(x = x, y = residualols, group = plotID, color = "OLS"), alpha = 0.1) +
  geom_line(data = eyefitting_model_data, aes(x = x, y = residualpca, group = plotID, color = "PCA"), alpha = 0.1) +
  geom_ribbon(aes(ymin = ols.lower, ymax = ols.upper, fill = "OLS"), color = NA, alpha = 0.5) +
  geom_line(aes(y = ols.pred, color = "OLS")) +
  geom_ribbon(aes(ymin = pca.lower, ymax = pca.upper, fill = "PCA"), color = NA, alpha = 0.5) +
  geom_line(aes(y = pca.pred, color = "PCA")) +
  geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.5) +
  facet_wrap(~`Parameter Choice`, labeller = labeller(`Parameter Choice` = label_both), scales = "free") +
  theme_bw(base_size = 14) +
  theme(aspect.ratio = 1,
        legend.position = "right",
        plot.title   = element_text(size = 12, hjust = 0),
        axis.text    = element_text(size = 12),
        axis.title   = element_text(size = 12),
        legend.title = element_text(size = 12),
        legend.text  = element_text(size = 12),
        # strip.text = element_text(size = 5, margin = margin(0.05,0,0.05,0, "cm")),
        # strip.background = element_rect(size = 0.5),
        legend.key.size = unit(1, "line")
        ) +
  scale_y_continuous("Residual") +
  scale_color_manual("Individual participant \nresiduals", values = c("steelblue", "orange"), labels = c("OLS", "PCA")) +
  scale_fill_manual("GAMM fitted trend", values = c("steelblue", "orange"), labels = c("OLS", "PCA")) 
eyefitting.gamm.plot
```

# Discussion and Conclusion

The intent of this paper was to establish you draw it as a tool. 
Eye Fitting Straight Lines in the Modern Era replicated the results found in @mosteller1981eye.
When shown points following a linear trend, participants tended to fit the slope of the first principal component over the slope of the least squares regression line.
This trend was most prominent when shown data simulated with larger variances.
The reproducibility of these results serve as evidence of the reliability of the you draw it method.

# Future Work

+ Use the method.
+ Write R package.

